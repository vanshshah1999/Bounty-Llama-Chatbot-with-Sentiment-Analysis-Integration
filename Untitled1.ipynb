{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzydOHVxSV3Nq2QGDsEhW3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Customer Support Chatbot with Sentiment Analysis\n","\n","This project focuses on creating a sentiment-aware **Customer Support Chatbot** using the Llama 2 model and a sentiment analysis model from Hugging Face. The chatbot is designed to assist customers by dynamically adjusting its responses based on their emotional state. If the sentiment of the customer's query is detected as **negative** (e.g., frustration or dissatisfaction), the bot provides a more empathetic and apologetic response, offering additional support. Conversely, for **positive** sentiments, it reinforces satisfaction and offers further suggestions or services. The bot aims to improve user experience by tailoring interactions to each user's mood, enhancing both customer satisfaction and service efficiency."],"metadata":{"id":"SP-LD_NNk4ZI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcCu4tXRio6b"},"outputs":[],"source":["!pip install -q accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub gradio"]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from huggingface_hub import login\n","import torch"],"metadata":{"id":"Lh-DUOSei18c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["api_key = \"your_hugging_face_token\"\n","if not api_key:\n","    raise ValueError(\"No API key found\")\n","\n","login(token=api_key)"],"metadata":{"id":"iG-HbLZ-i6hS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize CSV file for customer support QA dataset\n","csv_file = 'customer_support_qa_dataset.csv'\n","if not os.path.exists(csv_file):\n","    qa_data = {\n","        'question': [\n","            \"Where is my order?\",\n","            \"The product I received is defective.\",\n","            \"I want to cancel my order.\",\n","            \"Why is my order delayed?\",\n","            \"How do I return a product?\",\n","            \"I need help with a refund.\"\n","        ],\n","        'answer': [\n","            \"Your order is on the way and will be delivered by tomorrow.\",\n","            \"We're sorry to hear that! Please provide more details so we can arrange a replacement.\",\n","            \"We understand. Your order has been canceled. Is there anything else we can assist you with?\",\n","            \"We apologize for the delay. We are doing our best to get your order delivered as soon as possible.\",\n","            \"You can return the product within 30 days of purchase. Would you like assistance with this?\",\n","            \"We are processing your refund. Please allow 5-7 business days for the amount to reflect in your account.\"\n","        ]\n","    }\n","    qa_df = pd.DataFrame(qa_data)\n","    qa_df.to_csv(csv_file, index=False)\n","else:\n","    qa_df = pd.read_csv(csv_file)"],"metadata":{"id":"6vuCFGTmjVxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Llama 2 model and tokenizer\n","model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.use_default_system_prompt = False\n","\n","# Initialize Llama 2 pipeline\n","llama_pipeline = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    max_length=1024,\n",")"],"metadata":{"id":"0_IjTQuri_rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize sentiment analysis pipeline\n","sentiment_analyzer = pipeline(\"sentiment-analysis\")\n","# Define a system prompt\n","system_prompt = \"You are a helpful customer support assistant. Please provide concise and accurate answers to the questions.\"\n","\n","def answer_question(question):\n","    \"\"\"\n","    Answers a question using the customer support QA dataset or Llama 2 model.\n","\n","    Args:\n","    question (str): The input question from the user.\n","\n","    Returns:\n","    str: The answer to the question.\n","    \"\"\"\n","    global qa_df\n","    answer = qa_df[qa_df['question'].str.lower() == question.lower()]['answer']\n","\n","    if not answer.empty:\n","        return f\"Answer from QA dataset: {answer.iloc[0]}\"\n","    else:\n","        # Prepend the system prompt to the question\n","        input_text = f\"System Prompt:{system_prompt}\\nUser: {question}\\nAssistant:\"\n","        response = llama_pipeline(input_text, max_length=150, do_sample=True)[0]['generated_text']\n","\n","        # Extract the response\n","        response = response.split(\"Assistant:\")[1].strip()\n","\n","        if not any(qa_df['question'].str.lower() == question.lower()):\n","            new_row = pd.DataFrame({'question': [question], 'answer': [response]})\n","            qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n","            qa_df.to_csv(csv_file, index=False)\n","            return f\"Answer from Llama 2: {response} \\n(New QA pair added to the dataset.)\"\n","        return f\"Answer from Llama 2: {response}\""],"metadata":{"id":"khNYXAwWjaJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_sentiment(user_input):\n","    \"\"\"\n","    Analyzes the sentiment of the user input.\n","\n","    Args:\n","    user_input (str): The input text from the user.\n","\n","    Returns:\n","    tuple: A tuple containing the sentiment label and score.\n","    \"\"\"\n","    result = sentiment_analyzer(user_input)[0]\n","    label = result['label']\n","    score = result['score']\n","    return label, score"],"metadata":{"id":"VsOvWhXdjeol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentiment_aware_response(question):\n","    \"\"\"\n","    Generates a sentiment-aware response to the user's question.\n","\n","    Args:\n","    question (str): The input question from the user.\n","\n","    Returns:\n","    str: A sentiment-adjusted response to the question.\n","    \"\"\"\n","    label, score = analyze_sentiment(question)\n","\n","    if label == \"NEGATIVE\":\n","        sentiment_adjustment = \"I sense some frustration. Let me try to help: \"\n","    elif label == \"POSITIVE\":\n","        sentiment_adjustment = \"I'm glad you're in a good mood! Here's what I found: \"\n","    else:\n","        sentiment_adjustment = \"\"\n","\n","    main_response = answer_question(question)\n","    full_response = f\"{sentiment_adjustment}{main_response}\"\n","\n","    return full_response"],"metadata":{"id":"2VsihpYrjhoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gradio Interface\n","interface = gr.Interface(\n","    fn=sentiment_aware_response,\n","    inputs=\"text\",\n","    outputs=\"text\",\n","    title=\"Customer Support Chatbot with Sentiment Analysis\",\n","    description=\"Ask a customer support question, and the chatbot will analyze the sentiment, adjust the response, and provide an answer using the QA dataset or Llama 2.\",\n","    examples=[\n","        [\"Where is my order?\"],\n","        [\"The product I received is defective.\"],\n","        [\"I want to cancel my order.\"],\n","        [\"Why is my order delayed?\"],\n","        [\"I'm happy with the product!\"],\n","        [\"The service has been terrible!\"]\n","    ],\n","    theme=\"huggingface\"\n",")\n","\n","# Launch the Gradio Interface\n","interface.launch()"],"metadata":{"id":"WlPIvPuKjofX"},"execution_count":null,"outputs":[]}]}